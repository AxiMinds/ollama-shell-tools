from modules.llm_interface import get_llm_command
from modules.multi_model import get_multi_model_command
from modules.utils import execute_shell_command, clean_and_validate_command
from modules.cache_manager import cache_manager
import time

def handle_llm_input(command, llm_config, display, max_iterations=3):
    """Handles commands generated by LLMs with improved user feedback."""
    start_time = time.time()
    
    use_cache = not command.endswith('-nc')
    if not use_cache:
        command = command[:-3].strip()
    
    for model_key, details in llm_config['llms'].items():
        if command.startswith(details['name']):
            instruction = command[len(details['name']):].strip()
            
            display.show_message("Press CTRL-C to cancel the operation.")
            
            try:
                generated_command = get_llm_command(instruction, details, start_time, display, use_cache=use_cache)
                return process_command(generated_command, start_time, display)
            except KeyboardInterrupt:
                display.show_message("\nOperation cancelled by user.")
                return

    for multi_model_key, multi_model_details in llm_config['multi_model'].items():
        if command.startswith(multi_model_details['name']):
            instruction = command[len(multi_model_details['name']):].strip()
            
            display.show_message(f"\nUsing multi-model approach with {multi_model_details['decision_model']} as decision maker")
            generated_command = get_multi_model_command(instruction, llm_config, multi_model_details, start_time, display, use_cache)
            return process_command(generated_command, start_time, display)

    if command == "--clearcache":
        confirm = display.get_input("Are you sure you want to clear all cache? (y/n): ")
        if confirm.lower() == 'y':
            cache_manager.clear_cache()
            display.show_message("Cache cleared successfully.")
        return

    display.show_message("Error: The input did not match any known LLM prefix.")
    return

def process_command(generated_command, start_time, display):
    if generated_command.startswith("Error:"):
        display.show_message(generated_command)
        return

    generated_command = clean_and_validate_command(generated_command)

    if generated_command:
        display.show_message(f"\nLLM-generated Command: {generated_command}")
        confirm = display.get_input("Press Enter to execute, 'e' to edit, or 'n' to cancel: ").lower()

        if confirm == 'n':
            display.show_message("Command execution cancelled.")
            return
        elif confirm == 'e':
            generated_command = display.get_input("Edit the command: ")

        output, error = execute_shell_command(generated_command)
        if output:
            display.show_message(f"Output:\n{output}")
        if error:
            display.show_message(f"Error:\n{error}")
    else:
        display.show_message("\nFailed to generate a valid command.")
        display.show_message("Please try rephrasing your instruction.")

    display.show_message(f"Total execution time: {time.time() - start_time:.2f}s")
