from modules.llm_interface import get_llm_command, get_multi_model_command
from modules.utils import execute_shell_command, clean_and_validate_command
from modules.cache_manager import cache_manager
import time

def handle_llm_input(command, llm_config, max_iterations=3):
    """Handles commands generated by LLMs with improved user feedback."""
    start_time = time.time()
    
    use_cache = not command.endswith('-nc')
    if not use_cache:
        command = command[:-3].strip()
    
    for model_key, details in llm_config['llms'].items():
        if command.startswith(details['name']):
            instruction = command[len(details['name']):].strip()
            
            print("Press CTRL-C to cancel the operation.")
            
            try:
                if details['model'] in ['multi_llama', 'multi_phi']:
                    print(f"\nUsing multi-model approach with {details['decision_model']} as decision maker")
                    generated_command = get_multi_model_command(instruction, llm_config, details['decision_model'], start_time, use_cache)
                else:
                    system_prompt = details['prompts']['system']
                    user_prompt = details['prompts']['default']

                    print(f"\nUsing model: {model_key}")
                    print(f"Server: {details['url']}:{details['port']}")
                    print(f"System prompt: {system_prompt}")
                    print(f"Default prompt: {user_prompt}")
                    print(f"User instruction: {instruction}")
                    print(f"Additional metadata: use_json={details.get('use_json', False)}")

                    generated_command = get_llm_command(instruction, details, start_time, use_cache=use_cache)
                
                return process_command(generated_command, start_time)
            except KeyboardInterrupt:
                print("\nOperation cancelled by user.")
                return

    if command == "--clearcache":
        confirm = input("Are you sure you want to clear all cache? (y/n): ")
        if confirm.lower() == 'y':
            cache_manager.clear_cache()
            print("Cache cleared successfully.")
        return

    print("Error: The input did not match any known LLM prefix.")
    return

def process_command(generated_command, start_time):
    if generated_command.startswith("Error:"):
        print(generated_command)
        return

    generated_command = clean_and_validate_command(generated_command)

    if generated_command:
        print(f"\nLLM-generated Command: {generated_command}")
        confirm = input("Press Enter to execute, 'e' to edit, or 'n' to cancel: ").lower()

        if confirm == 'n':
            print("Command execution cancelled.")
            return
        elif confirm == 'e':
            generated_command = input("Edit the command: ")

        output, error = execute_shell_command(generated_command)
        if output:
            print(f"Output:\n{output}")
        if error:
            print(f"Error:\n{error}")
    else:
        print("\nFailed to generate a valid command.")
        print("Please try rephrasing your instruction.")

    print(f"Total execution time: {time.time() - start_time:.2f}s")
